{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5257a5-eb4a-4638-8172-c4f5a0b92f6b",
   "metadata": {},
   "source": [
    "# A workflow that:\n",
    "- 1.) Scrapes news headlines\n",
    "- 2.) Performs sentiment analysis on the headlines with up to 4 different models\n",
    "- 3.) Ensembles the sentimennt analyses\n",
    "- 4.) Combines sentiment analysis with numeric stock data\n",
    "- 5.) Gives the comined data to 3 different Ml models to forcast stock price(s)\n",
    "- 6.) Ensembles the forcasts\n",
    "- 7.) Pushes the forcast(s) via telegram notification with an option to continuously run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72ec9d-6311-4dca-a8f5-212c3baf3135",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a0cf9-b9a1-4b8c-8368-de6179eb32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install torch\n",
    "!pip install googlesearch-python\n",
    "!pip install GoogleNews\n",
    "!pip install textblob\n",
    "!pip install schedule\n",
    "!pip install time\n",
    "!pip install telebot\n",
    "!pip install telethon\n",
    "!pip install transformers\n",
    "!pip install vadersentiment\n",
    "!pip install alpha_vantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae9a88-6aa2-4c92-b0ba-f7d45e81e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import schedule\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from GoogleNews import GoogleNews\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9608631",
   "metadata": {},
   "source": [
    "## Telegram Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Include Telegram bot token and chat ID for pushing forecast results\n",
    "BOT_TOKEN = \"<BOT_TOKEN>\"\n",
    "chat_id = \"<CHAT_ID>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092d7b1-7c85-49e2-960c-57497ea9c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ GOOGLE NEWS LIMITS TO 10 ARTICLES ###################\n",
    "# Immediately lends itself to event-based classification, or use another source\n",
    "def scrape_news_headlines(ticker):\n",
    "    gn = GoogleNews(lang='en')\n",
    "    gn.search(ticker)\n",
    "    news_articles = gn.results()\n",
    "    titles = [article['title'] for article in news_articles]\n",
    "    return titles\n",
    "\n",
    "#ticker = 'CURLF'\n",
    "#articles = scrape_news_headlines(ticker)\n",
    "#print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525d8c1-cb77-4fa7-b3cf-c61d3d3d7958",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187ac95-dab1-46ba-9978-60179e324356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6c364-9ba9-44d3-a816-73586d1fc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to have access to LLama -  via hugging face\n",
    "### THIS IS ABlE TO RUN ON a T4!!!\n",
    "# https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster\n",
    "# https://github.com/AI4Finance-Foundation/FinGPT/blob/master/FinGPT_Inference_Llama2_13B_falcon_7B_for_Beginners.ipynb\n",
    "### see how the flow works for these, look at the hugging face page dataset for training.\n",
    "\n",
    "\n",
    "\n",
    "#### note, this is for the forcaster FinGPT model, which is not specifically for sentiment analysis\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,   # optional if you have enough VRAM\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "\n",
    "gpt_model = PeftModel.from_pretrained(base_model, 'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora')\n",
    "gpt_model = gpt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a82319-7fc2-4bbc-a534-3452b803ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# look at FinGPT_sentiment ##############\n",
    "\n",
    "def gpt_sentiment_analysis(articles):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gpt_model.to(device)  # Move the model to the device\n",
    "\n",
    "    sentiments = []\n",
    "    for article in articles:\n",
    "        inputs = tokenizer(article, return_tensors='pt', max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  # Move the input tensors to the device\n",
    "        outputs = gpt_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "        # Get sentiment scores and normalize between 0 and 1\n",
    "        sentiment_scores = torch.sigmoid(outputs.logits)[0]  # Extract scores from tensor\n",
    "        avg_sentiment = torch.mean(sentiment_scores)  # Calculate average sentiment score\n",
    "        normalized_sentiment = (avg_sentiment - torch.min(sentiment_scores)) / (torch.max(sentiment_scores) - torch.min(sentiment_scores))  # Normalize between 0 and 1\n",
    "        sentiments.append(normalized_sentiment.cpu().item())  # Convert to scalar value and append to list\n",
    "\n",
    "    # Create a Pandas DataFrame with the sentiment scores\n",
    "    headlines_df = pd.DataFrame({'title': articles, 'sentiment': sentiments})\n",
    "\n",
    "    return headlines_df\n",
    "\n",
    "#ticker = 'AAPL'\n",
    "#articles = scrape_news_headlines(ticker)\n",
    "#sentiments = gpt_sentiment_analysis(articles)\n",
    "#sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd06692-8562-44a2-a78b-7517bb901522",
   "metadata": {},
   "source": [
    "# Get News -- either google news or News API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953fcc2-17be-4cdf-a106-28893e4207aa",
   "metadata": {},
   "source": [
    "# Sentiment Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28166006-c9e3-47c0-a259-77a39ef3d364",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a98c63-cf3e-4c5a-a7df-27edbdc32285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_sentiment_analysis(headlines):\n",
    "    # Convert the list of headlines to a pandas DataFrame\n",
    "    headlines_df = pd.DataFrame({'title': headlines})\n",
    "\n",
    "    # Load pre-trained RoBERTa model and tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "    # Preprocess headlines data\n",
    "    encoded_headlines = headlines_df['title'].apply(lambda x: tokenizer.encode_plus(x,\n",
    "                                                                                  add_special_tokens=True,\n",
    "                                                                                  max_length=512,\n",
    "                                                                                  return_attention_mask=True,\n",
    "                                                                                  return_tensors='pt',\n",
    "                                                                                  truncation=True))\n",
    "\n",
    "    # Extract the input_ids and attention_mask from the encoded headlines\n",
    "    input_ids = encoded_headlines.apply(lambda x: x['input_ids'].flatten().tolist())\n",
    "    attention_mask = encoded_headlines.apply(lambda x: x['attention_mask'].flatten().tolist())\n",
    "\n",
    "    # Create new columns in the headlines DataFrame for input_ids and attention_mask\n",
    "    headlines_df['input_ids'] = input_ids\n",
    "    headlines_df['attention_mask'] = attention_mask\n",
    "\n",
    "    # Create a list to store sentiment scores\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # Perform sentiment analysis on each headline\n",
    "    for input_id, attention_mask in zip(headlines_df['input_ids'], headlines_df['attention_mask']):\n",
    "        # Create a tensor dataset and data loader\n",
    "        input_id_tensor = torch.tensor([input_id])\n",
    "        attention_mask_tensor = torch.tensor([attention_mask])\n",
    "        dataset = torch.utils.data.TensorDataset(input_id_tensor, attention_mask_tensor)\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                input_id, attention_mask = batch\n",
    "                outputs = model(input_id, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                sentiment_score = torch.softmax(logits, dim=1)[0][1].item()\n",
    "                sentiment_scores.append(sentiment_score)\n",
    "\n",
    "    # Normalize sentiment scores to be between 0 and 1\n",
    "    min_score = min(sentiment_scores)\n",
    "    max_score = max(sentiment_scores)\n",
    "    normalized_sentiment_scores = [(score - min_score) / (max_score - min_score) for score in sentiment_scores]\n",
    "\n",
    "    # Add normalized sentiment scores to the dataframe\n",
    "    headlines_df['sentiment'] = normalized_sentiment_scores\n",
    "    headlines_df.drop('attention_mask', axis = 1, inplace = True)\n",
    "\n",
    "    return headlines_df\n",
    "\n",
    "# Example usage\n",
    "#ticker = 'AAPL'\n",
    "#headlines = scrape_news_headlines(ticker)\n",
    "#sentiments = roberta_sentiment_analysis(headlines)\n",
    "#sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c26546-8960-4496-b1e3-664b767d52fd",
   "metadata": {},
   "source": [
    "## Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38e2fa-45e4-4644-9898-59760a129ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VaderSentiment\n",
    "def vader_sentiment_analysis(headlines):\n",
    "    # Convert the list of headlines to a pandas DataFrame\n",
    "    headlines_df = pd.DataFrame({'title': headlines})\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Apply sentiment analysis to each headline\n",
    "    headlines_df['sentiment'] = headlines_df['title'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Normalize sentiment scores to be between 0 and 1\n",
    "    headlines_df['sentiment'] = headlines_df['sentiment'].apply(lambda x: (x + 1) / 2)\n",
    "\n",
    "    return headlines_df\n",
    "\n",
    "#ticker = 'AAPL'\n",
    "#headlines = scrape_news_headlines(ticker)\n",
    "#sentiments = vader_sentiment_analysis(headlines)\n",
    "#sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7d498-994b-4e8f-b973-d5a0c3a254bc",
   "metadata": {},
   "source": [
    "## Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b3583-d341-4fc0-b329-4d704bde5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "def blob_sentiment_analysis(headlines):\n",
    "    # Convert the list of headlines to a pandas DataFrame\n",
    "    headlines_df = pd.DataFrame({'title': headlines})\n",
    "\n",
    "    # Create a new column to store the sentiment scores\n",
    "    headlines_df['sentiment'] = headlines_df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Normalize the sentiment scores to a range of 0 to 1\n",
    "    min_sentiment = headlines_df['sentiment'].min()\n",
    "    max_sentiment = headlines_df['sentiment'].max()\n",
    "    headlines_df['sentiment_normalized'] = (headlines_df['sentiment'] - min_sentiment) / (max_sentiment - min_sentiment)\n",
    "\n",
    "    return headlines_df\n",
    "\n",
    "#ticker = 'AAPL'\n",
    "#headlines = scrape_news_headlines(ticker)\n",
    "#sentiments = blob_sentiment_analysis(headlines)\n",
    "#sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fa14f-8ee1-465b-8408-b0a296f6648c",
   "metadata": {},
   "source": [
    "# Ensemble Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e317752-e6de-44a3-a882-906ce163d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_sentiment_analysis(headlines, include_gpt):\n",
    "    # Initialize a list to store the sentiment scores from each model\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # VaderSentiment\n",
    "    vader_scores = vader_sentiment_analysis(headlines)['sentiment']\n",
    "    sentiment_scores.append(vader_scores)\n",
    "\n",
    "    # RoBERTa\n",
    "    roberta_scores = roberta_sentiment_analysis(headlines)['sentiment']\n",
    "    sentiment_scores.append(roberta_scores)\n",
    "\n",
    "    # TextBlob\n",
    "    textblob_scores = blob_sentiment_analysis(headlines)['sentiment']\n",
    "    sentiment_scores.append(textblob_scores)\n",
    "\n",
    "    # GPT\n",
    "    if include_gpt:\n",
    "        gpt_scores = gpt_sentiment_analysis(headlines)['sentiment']\n",
    "        sentiment_scores.append(gpt_scores)\n",
    "\n",
    "    # Create a DataFrame to store the ensemble sentiment scores\n",
    "    ensemble_df = pd.DataFrame({'headline': headlines})\n",
    "\n",
    "    # Calculate the average sentiment score across all models\n",
    "    ensemble_df['ensemble_sentiment'] = np.mean(sentiment_scores, axis=0)\n",
    "\n",
    "    # Calculate the standard deviation of sentiment scores across all models\n",
    "    ensemble_df['ensemble_sentiment_std'] = np.std(sentiment_scores, axis=0)\n",
    "\n",
    "    return ensemble_df\n",
    "\n",
    "#ticker = 'AAPL'\n",
    "#headlines = scrape_news_headlines(ticker)\n",
    "#sentiments = ensemble_sentiment_analysis(headlines, include_gpt=False)\n",
    "#sentiments.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8a95-528f-45c7-9b9f-904f97e28d71",
   "metadata": {},
   "source": [
    "# Sentiment Pipieline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0127f-6ba0-4bac-ba59-156acbaaae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sentiment_ensemble_pipeline(ticker, include_gpt):\n",
    "  headlines = scrape_news_headlines(ticker)\n",
    "  sentiments = ensemble_sentiment_analysis(headlines, include_gpt=include_gpt)\n",
    "  return sentiments\n",
    "\n",
    "scrape_sentiment_ensemble_pipeline(ticker = 'TSLA', include_gpt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94b261-af09-4d2b-aab0-8c2242e1dbad",
   "metadata": {},
   "source": [
    "# Numeric models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fe963-560e-49ad-9257-14cf9c5d1b0e",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215aa07-cce8-41f9-b899-a932ea64f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_data(ticker):\n",
    "    # Define the ticker symbol and time period\n",
    "    period = \"3mo\" #must be one of ['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max']\"\n",
    "    interval = \"1h\"\n",
    "\n",
    "    # Retrieve the historical market data\n",
    "    data = yf.download(tickers=ticker, period=period, interval=interval)\n",
    "\n",
    "    # Retrieve the ticker info\n",
    "    ticker_info = yf.Ticker(ticker).info\n",
    "\n",
    "    # Extract the market cap\n",
    "    market_cap = ticker_info['marketCap']\n",
    "\n",
    "    # Calculate the moving averages\n",
    "    data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "    data['MA_200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "    # Calculate the RSI\n",
    "    delta = data['Close'].diff(1)\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    roll_up = up.rolling(window=14).mean()\n",
    "    roll_down = down.rolling(window=14).mean().abs()\n",
    "    RS = roll_up / roll_down\n",
    "    data['RSI'] = 100.0 - (100.0 / (1.0 + RS))\n",
    "\n",
    "    # Create a single dataframe called numeric_df\n",
    "    numeric_df = data.select_dtypes(include=['number'])\n",
    "\n",
    "    # Reset the index\n",
    "    numeric_df.reset_index(inplace=True)\n",
    "\n",
    "    return numeric_df\n",
    "\n",
    "#ticker = 'AAPL'\n",
    "#numeric = get_numeric_data(ticker)\n",
    "#numeric.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b9bd9-f48c-4bd8-b109-0a7a48ee99cd",
   "metadata": {},
   "source": [
    "## Concat with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56db359-e7de-44e9-b6b7-68c82cd4984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_numeric_sentiments(numeric_df, sentiments_df):\n",
    "  # Concatenate the two dataframes along the row axis\n",
    "  combined_df = pd.concat([numeric_df, sentiments_df], axis=1)\n",
    "\n",
    "  # Rename the columns to avoid any conflicts\n",
    "  combined_df = combined_df.rename(columns={'Open': 'numeric_Open',\n",
    "                                          'High': 'numeric_High',\n",
    "                                          'Low': 'numeric_Low',\n",
    "                                          'Close': 'numeric_Close',\n",
    "                                          'Adj Close': 'numeric_Adj Close',\n",
    "                                          'Volume': 'numeric_Volume',\n",
    "                                          'MA_50': 'numeric_MA_50',\n",
    "                                          'MA_200': 'numeric_MA_200',\n",
    "                                          'RSI': 'numeric_RSI'})\n",
    "\n",
    "  # Add a timestamp column to the sentiments_df\n",
    "  sentiments_df['timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "  # Concatenate the two dataframes along the row axis\n",
    "  combined_df = pd.concat([numeric_df, sentiments_df], axis=1)\n",
    "\n",
    "  # Drop any duplicate columns\n",
    "  combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "\n",
    "  # Reset the index of the combined dataframe\n",
    "  combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "  return combined_df\n",
    "\n",
    "ticker = 'AAPL'\n",
    "numeric_df = get_numeric_data(ticker)\n",
    "sentiments_df = scrape_sentiment_ensemble_pipeline(ticker = ticker, include_gpt=False)\n",
    "combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "combined_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947304f-91f5-442a-85f9-e44f19b93baa",
   "metadata": {},
   "source": [
    "## prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d516972-2739-4471-a583-9e103b6c3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_forecast(df, future_days):\n",
    "    # Select only the necessary columns\n",
    "    df = df[['Datetime', 'Close']]\n",
    "\n",
    "    # Rename the columns to match Prophet's requirements\n",
    "    df = df.rename(columns={'Datetime': 'ds', 'Close': 'y'})\n",
    "\n",
    "    # Ensure the 'ds' column is of datetime type and remove timezone\n",
    "    df['ds'] = pd.to_datetime(df['ds']).dt.tz_localize(None)\n",
    "\n",
    "    # Create a Prophet model\n",
    "    model = Prophet()\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(df)\n",
    "\n",
    "    # Make a forecast for the next 'future_days' days\n",
    "    future = model.make_future_dataframe(periods=future_days)\n",
    "\n",
    "    # Make predictions on the future dataframe\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Extract the forecasted close prices\n",
    "    future_close_prices = forecast['yhat'].values[-future_days:]\n",
    "    #model.plot(forecast)\n",
    "    return future_close_prices\n",
    "\n",
    "ticker = 'CURlF'\n",
    "combined_df = get_numeric_data(ticker)\n",
    "future_close_prices = prophet_forecast(combined_df, future_days = 30)\n",
    "print(\"Future close prices:\", future_close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42c8a0-d7f0-468c-99b9-bb0c544fcc24",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb6b3-03a4-4c9b-87b3-d93eb27589d0",
   "metadata": {},
   "source": [
    "### LSTM Trian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f96aec-6f89-445a-a96e-0e809916f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_lstm_forcast(df):\n",
    "\n",
    "  # Load the data\n",
    "  df = df.copy()\n",
    "  df.drop('headline', axis = 1, inplace = True)\n",
    "\n",
    "  # Convert the Datetime column to datetime format\n",
    "  df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "  # Move the Datetime column to the first position\n",
    "  cols = df.columns.tolist()\n",
    "  cols.insert(0, cols.pop(cols.index('Datetime')))\n",
    "  df = df[cols]\n",
    "\n",
    "  # Create a new dataframe with only the relevant columns\n",
    "  relevant_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                    'MA_50', 'MA_200', 'RSI',\n",
    "                  'ensemble_sentiment', 'ensemble_sentiment_std']\n",
    "  data_df = df[relevant_cols]\n",
    "\n",
    "  # Drop any rows with missing values\n",
    "  #df_resampled.dropna(inplace=True)\n",
    "\n",
    "  # Create a new dataframe with only the Close column\n",
    "  close_df = data_df[['Close']]\n",
    "  close_df.set_index(df['Datetime'], inplace=True)\n",
    "\n",
    "  # Scale the data using Min-Max Scaler\n",
    "  scaler = MinMaxScaler(feature_range=(0,1))\n",
    "  close_scaled = scaler.fit_transform(close_df)\n",
    "\n",
    "  # Create a function to create the LSTM dataset\n",
    "  def create_lstm_dataset(X, y, time_steps=1):\n",
    "      Xs, ys = [], []\n",
    "      for i in range(len(X)-time_steps):\n",
    "          Xs.append(X[i:(i+time_steps)])\n",
    "          ys.append(y[i+time_steps])\n",
    "      return np.array(Xs), np.array(ys)\n",
    "\n",
    "  # Create the LSTM dataset\n",
    "  X, y = create_lstm_dataset(close_scaled, close_scaled, time_steps=60)\n",
    "\n",
    "  # Reshape the data for LSTM\n",
    "  X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "  # Create the LSTM model\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(LSTM(units=50, return_sequences=False))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(units=1))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "  model.save_weights('lstm_model.h5')\n",
    "\n",
    "  # Make predictions on the last 60 hours of data\n",
    "  last_60_hours = close_scaled[-60:]\n",
    "  last_60_hours = last_60_hours.reshape(1, 60, 1)\n",
    "  prediction = model.predict(last_60_hours)\n",
    "\n",
    "  # Inverse transform the prediction\n",
    "  prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "  # Print the predicted close price\n",
    "  print(\"Predicted Close Price:\", prediction[0][0])\n",
    "\n",
    "  # Plot the predicted close price\n",
    "  plt.plot(close_df.index[-60:], close_df.values[-60:])\n",
    "  plt.plot([close_df.index[-1] + pd.Timedelta(hours=1)], [prediction[0][0]], 'ro', label='Predicted Close Price')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  return prediction\n",
    "\n",
    "#combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "#train_lstm_forcast(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985918a4-2cd3-43d1-9949-4b3c139938d9",
   "metadata": {},
   "source": [
    "### LSTM Forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82256c-55a4-45d2-a9d5-a320dc4c10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forcast(df, future_days):\n",
    "  # Load the data\n",
    "  df = df.copy()\n",
    "  df.drop('headline', axis = 1, inplace = True)\n",
    "\n",
    "  # Convert the Datetime column to datetime format\n",
    "  df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "  # Move the Datetime column to the first position\n",
    "  cols = df.columns.tolist()\n",
    "  cols.insert(0, cols.pop(cols.index('Datetime')))\n",
    "  df = df[cols]\n",
    "\n",
    "  # Create a new dataframe with only the relevant columns\n",
    "  relevant_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                    'MA_50', 'MA_200', 'RSI',\n",
    "                  'ensemble_sentiment', 'ensemble_sentiment_std']\n",
    "  data_df = df[relevant_cols]\n",
    "\n",
    "  # Drop any rows with missing values\n",
    "  #df_resampled.dropna(inplace=True)\n",
    "\n",
    "  # Create a new dataframe with only the Close column\n",
    "  close_df = data_df[['Close']]\n",
    "  close_df.set_index(df['Datetime'], inplace=True)\n",
    "\n",
    "  # Scale the data using Min-Max Scaler\n",
    "  scaler = MinMaxScaler(feature_range=(0,1))\n",
    "  close_scaled = scaler.fit_transform(close_df)\n",
    "\n",
    "  # Create a function to create the LSTM dataset\n",
    "  def create_lstm_dataset(X, y, time_steps=1):\n",
    "      Xs, ys = [], []\n",
    "      for i in range(len(X)-time_steps):\n",
    "          Xs.append(X[i:(i+time_steps)])\n",
    "          ys.append(y[i+time_steps])\n",
    "      return np.array(Xs), np.array(ys)\n",
    "\n",
    "  # Create the LSTM dataset\n",
    "  X, y = create_lstm_dataset(close_scaled, close_scaled, time_steps=60)\n",
    "\n",
    "  # Reshape the data for LSTM\n",
    "  X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "  # Create the LSTM model\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(LSTM(units=50, return_sequences=False))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(units=1))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  # Load the trained model\n",
    "  model.load_weights('lstm_model.h5')\n",
    "\n",
    "  # Prepare the input data for forecasting\n",
    "  last_60_hours = close_scaled[-60:]\n",
    "  future_input = last_60_hours.reshape(1, 60, 1)\n",
    "\n",
    "  # Forecast future close prices\n",
    "  future_close_prices = []\n",
    "  for i in range(future_days):\n",
    "    prediction = model.predict(future_input)\n",
    "    prediction = scaler.inverse_transform(prediction)\n",
    "    future_close_prices.append(prediction[0, 0])\n",
    "    future_input = np.append(future_input[:, 1:, :], prediction.reshape((1, 1, 1)), axis=1)\n",
    "\n",
    "  return future_close_prices\n",
    "\n",
    "#combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "#future_close_prices = lstm_forcast(combined_df, future_days = 30)\n",
    "#print(\"Future close prices:\", future_close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851149d-d34b-41bf-8ec8-60b82da4045a",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699824cd-7e40-46f7-9ea0-59a04c25768a",
   "metadata": {},
   "source": [
    "### GRU Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908dc1f-aa77-4812-a096-8af8472bbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gru_forcast(df):\n",
    "    # Load the data\n",
    "  df = df.copy()\n",
    "  df.drop('headline', axis = 1, inplace = True)\n",
    "\n",
    "  # Convert the Datetime column to datetime format\n",
    "  df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "  # Move the Datetime column to the first position\n",
    "  cols = df.columns.tolist()\n",
    "  cols.insert(0, cols.pop(cols.index('Datetime')))\n",
    "  df = df[cols]\n",
    "\n",
    "  # Create a new dataframe with only the 'Close' column\n",
    "  close_df = df[['Close']]\n",
    "\n",
    "  # Scale the data\n",
    "  scaler = MinMaxScaler(feature_range=(0,1))\n",
    "  scaled_close = scaler.fit_transform(close_df)\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  train_size = int(len(scaled_close) * 0.8)\n",
    "  test_size = len(scaled_close) - train_size\n",
    "  train_close, test_close = scaled_close[0:train_size], scaled_close[train_size:len(scaled_close)]\n",
    "\n",
    "  # Convert the data into sequences\n",
    "  def create_sequences(data, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])  # Append the entire row\n",
    "        y.append(data[i+seq_len])  # Append the entire next row\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "  seq_len = 60\n",
    "  X_train, y_train = create_sequences(train_close, seq_len)\n",
    "  print(\"X_train shape:\", X_train.shape)\n",
    "  print(\"y_train shape:\", y_train.shape)\n",
    "  X_test, y_test = create_sequences(test_close, seq_len)\n",
    "  print(\"X_test shape:\", X_test.shape)\n",
    "  print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "  # Reshape the data for GRU\n",
    "  X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "  X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "  # Create the GRU model\n",
    "  model = Sequential()\n",
    "  model.add(GRU(units=50, return_sequences=True, input_shape=(seq_len, 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(GRU(units=50, return_sequences=False))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(units=1))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "  model.save_weights('gru_model.h5')\n",
    "\n",
    "  # Make predictions\n",
    "  predictions = model.predict(X_test)\n",
    "\n",
    "  # Rescale the predictions\n",
    "  predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "\n",
    "  # Rescale the actual values\n",
    "  y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "  print(\"Predictions (rescaled):\", predictions_rescaled)\n",
    "  print(\"Actual (rescaled):\", y_test_rescaled)\n",
    "\n",
    "  # Plot the results\n",
    "  plt.plot(y_test_rescaled, label='Actual')\n",
    "  plt.plot(predictions_rescaled, label='Predicted')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "#combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "#train_gru_forcast(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4626a15-5818-4665-a5a6-0e026a7ad82c",
   "metadata": {},
   "source": [
    "### GRU Forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83546221-65aa-4739-bab7-7ad14203bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_forcast(df, future_days):\n",
    "  # Load the data\n",
    "  df = df.copy()\n",
    "  df.drop('headline', axis = 1, inplace = True)\n",
    "\n",
    "  # Convert the Datetime column to datetime format\n",
    "  df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "  # Move the Datetime column to the first position\n",
    "  cols = df.columns.tolist()\n",
    "  cols.insert(0, cols.pop(cols.index('Datetime')))\n",
    "  df = df[cols]\n",
    "\n",
    "  # Create a new dataframe with only the 'Close' column\n",
    "  close_df = df[['Close']]\n",
    "\n",
    "  # Scale the data\n",
    "  scaler = MinMaxScaler(feature_range=(0,1))\n",
    "  scaled_close = scaler.fit_transform(close_df)\n",
    "\n",
    "  # Prepare the input data for the model\n",
    "  seq_len = 60\n",
    "  last_seq = scaled_close[-seq_len:]\n",
    "  future_input = last_seq.reshape((1, seq_len, 1))\n",
    "\n",
    "  # Create the GRU model\n",
    "  model = Sequential()\n",
    "  model.add(GRU(units=50, return_sequences=True, input_shape=(seq_len, 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(GRU(units=50, return_sequences=False))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(units=1))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  # Load the trained model\n",
    "  model.load_weights('gru_model.h5') ###############\n",
    "\n",
    "  # Make predictions\n",
    "  predictions = model.predict(future_input)\n",
    "\n",
    "  # Rescale the predictions\n",
    "  predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "\n",
    "  # Forecast future close prices\n",
    "  future_close_prices = []\n",
    "  for i in range(future_days):\n",
    "    future_input = np.append(future_input[:, 1:, :], predictions.reshape((1, 1, 1)), axis=1)\n",
    "    predictions = model.predict(future_input)\n",
    "    predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "    future_close_prices.append(predictions_rescaled[0, 0])\n",
    "\n",
    "  # plot future close prices\n",
    "  #plt.plot( future_close_prices, label='Predicted')\n",
    "  #plt.legend()\n",
    "  #plt.show()\n",
    "\n",
    "  return future_close_prices\n",
    "ticker = 'PG'\n",
    "include_gpt=False\n",
    "sentiments_df = scrape_sentiment_ensemble_pipeline(ticker, include_gpt=include_gpt)\n",
    "numeric_df = get_numeric_data(ticker)\n",
    "combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "future_close_prices = gru_forcast(combined_df, future_days = 1)\n",
    "print(\"Future close prices:\", future_close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd8257-cd7a-48be-9f29-4a0bcd82fe6d",
   "metadata": {},
   "source": [
    "## Ensemble Forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694e7c8-2b8f-4e8a-86a8-6ac892ed52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focast_ensemble(df, future_days):\n",
    "  prophet_preds = prophet_forecast(df, future_days)\n",
    "  print(prophet_preds)\n",
    "  lstm_preds = lstm_forcast(df, future_days=future_days)\n",
    "  print(lstm_preds)\n",
    "  gru_preds = gru_forcast(df, future_days=future_days)\n",
    "  print(gru_preds)\n",
    "  ensemble_forecast = (prophet_preds + lstm_preds + gru_preds) / 3\n",
    "  plt.plot(ensemble_forecast, label='Ensemble')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  return ensemble_forecast\n",
    "\n",
    "ticker = 'PG'\n",
    "include_gpt=False\n",
    "sentiments_df = scrape_sentiment_ensemble_pipeline(ticker, include_gpt=include_gpt)\n",
    "numeric_df = get_numeric_data(ticker)\n",
    "combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "future_close_prices = focast_ensemble(combined_df, future_days = 30)\n",
    "print(\"Future close prices:\", future_close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf9e5-819b-4570-9aa4-1de796381ae0",
   "metadata": {},
   "source": [
    "# Forecast Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db2ba6-3df4-4c0d-b626-a7317d579be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_pipeline(ticker, include_gpt, future_days):\n",
    "  sentiments_df = scrape_sentiment_ensemble_pipeline(ticker, include_gpt)\n",
    "  #print(sentiments_df.head())\n",
    "  numeric_df = get_numeric_data(ticker)\n",
    "  #print(numeric_df.head())\n",
    "  combined_df = concat_numeric_sentiments(numeric_df, sentiments_df)\n",
    "  #print(combined_df.head())\n",
    "  future_close_prices = focast_ensemble(combined_df, future_days)\n",
    "  return future_close_prices\n",
    "\n",
    "#ticker = 'TSLA'\n",
    "#include_gpt=False\n",
    "#future_days=30\n",
    "#forecast_pipeline(ticker, include_gpt, future_days)\n",
    "################ SIIIIICK #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfd092-fd74-4d5f-8398-966845884292",
   "metadata": {},
   "source": [
    "# Telegram Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333b370-c4b7-477e-9d2a-c1d22581921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_forcast(ticker, prediction):\n",
    "\n",
    "    message = f\"Predicted stock price for: \\n {ticker}: {round(prediction[0], 2)}\"\n",
    "\n",
    "    url = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage?chat_id={chat_id}&text={message}\"\n",
    "\n",
    "    requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ee139-5ac3-4c8c-8f23-870346d796c1",
   "metadata": {},
   "source": [
    "# WHOLE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039ed27-ac0c-42c1-90fc-38b5f1806f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_forcaster_aggregator():\n",
    "  ticker = 'HD'\n",
    "  include_gpt=False\n",
    "  future_days=1\n",
    "  prediction = forecast_pipeline(ticker, include_gpt=include_gpt, future_days=future_days)\n",
    "  send_forcast(ticker, prediction)\n",
    "  return prediction\n",
    "\n",
    "active_forcaster_aggregator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265588aa-d971-48be-b156-4db74dea910d",
   "metadata": {},
   "source": [
    "# Repeater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70a73e-3f25-420e-839c-0eee5be3da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline every specified interval\n",
    "schedule.clear()\n",
    "schedule.every(1).minutes.do(active_forcaster_aggregator)  # Run every 1 minute\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
